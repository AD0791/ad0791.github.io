{
  "hash": "75362c3c46dde060fab15a3061a839af",
  "result": {
    "markdown": "---\ntitle: Web Scraping\nsubtitle: Scrape the web to retrieve your data\ndescription: Python offers us multiple battle tested solutions to tackle this problem properly.\nauthor: Alexandro Disla\nauthor-meta: Data Analyst & backend dev\ndate: '2023-07-15'\ndate-modified: now\ndate-meta: time of the last article update\nreading-time: 10\nimage: ../../img/web_scraping.jpeg\nimage-alt: The logo of web scraping\ncategories:\n  - Python\n  - Web scraping\ntoc: true\ntoc-depth: 2\ntoc-expand: true\nformat:\n  html:\n    code-fold: true\n---\n\n# Web scraping\n\nWeb scraping is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.\n\nIf you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own and make sure that you’re not violating any Terms of Service before you start a large-scale project.\n\n## The need for Web scraping\n\nSay you’re a surfer, both online and in real life, and you’re looking for employment. However, you’re not looking for just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way!\n\nThere’s a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You think about checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time.\n\nThankfully, the world offers other ways to apply that surfer’s mindset! Instead of looking at the job site every day, you can use Python to help automate your job search’s repetitive parts. Automated web scraping can be a solution to speed up the data collection process. You write your code once, and it will get the information you want many times and from many pages.\n\nIn contrast, when you try to get the information you want manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and repetition.\n\nThere’s so much information on the Web, and new information is constantly added. You’ll probably be interested in at least some of that data, and much of it is just out there for the taking. Whether you’re actually on the job hunt or you want to download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals.\n\n## Available Options and Alternatives\n\n7 Best Python Libraries For Web Scraping\n\n### Quick comparison of the solutions\n\nBeautifulSoup\nFeatures of BeautifulSoup\nPros of BeautifulSoup\nCons of BeautifulSoup\nScrapy\nFeatures of Scrapy\nPros of Scrapy\nCons of Scrapy\nSelenium\nFeatures of Selenium\nPros of Selenium\nCons of Selenium\nRequests\nFeatures of Requests\nPros of Requests\nCons of Requests\nUrllib3\nFeatures of urllib3\nPros of urllib3\nCons of urllib3\nLxml\nFeatures of LXML\nPros of LXML\nCons of LXML\nMechanicalSoup\nFeatures of MechanicalSoup\nPros of MechanicalSoup\nCons of MechanicalSoup\n\n### An Alternative to Web Scraping: APIs\nSome website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\n\nWhen you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\n\nThe front-end presentation of a site might change often, but such a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data.\n\nHowever, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality.\n\nThe approach and tools you need to gather information using APIs are outside the scope of this tutorial. To learn more about it, check out API Integration in Python.\n\n\n> Now let's run a project with Beautiful Soup and scrapy\n\n# Beautiful Soup\n\nBeautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n\nThese instructions illustrate all major features of Beautiful Soup 4, with examples. I show you what the library is good for, how it works, how to use it, how to make it do what you want, and what to do when it violates your expectations.\n\nThis document covers Beautiful Soup version 4.12.1. The examples in this documentation were written for Python 3.8.\n\nYou might be looking for the documentation for Beautiful Soup 3. If so, you should know that Beautiful Soup 3 is no longer being developed and that all support for it was dropped on December 31, 2020. If you want to learn about the differences between Beautiful Soup 3 and Beautiful Soup 4, see Porting code to BS4.\n\n# Scrapy\n\nScrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n\nEven though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n\n# testing area\n\nwrite your code here\n\n## BS4\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom requests import get\nfrom bs4 import BeautifulSoup\nfrom pandas import DataFrame\n```\n:::\n\n\nThe scraping process of fake-jobs website:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nURL = \"https://realpython.github.io/fake-jobs/\"\npage = get(URL)\n\nsoup = BeautifulSoup(page.content, \"html.parser\")\nresults = soup.find(id=\"ResultsContainer\")\njob_elements = results.find_all(\"div\", class_=\"card-content\")\n\ndef handle_card_info(jobs):\n    data={\n        'title': [],\n        'company': [],\n        'location': []\n    }\n    for job_element in job_elements:\n        title_element = job_element.find(\"h2\", class_=\"title\")\n        company_element = job_element.find(\"h3\", class_=\"company\")\n        location_element = job_element.find(\"p\", class_=\"location\")\n        if title_element:\n            data[\"title\"].append(title_element.text.strip())\n        if company_element:\n            data[\"company\"].append(company_element.text.strip())\n        if company_element:\n            data[\"location\"].append(location_element.text.strip())\n    return data\n\n```\n:::\n\n\nMake a DataFrame:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncard_data = handle_card_info(job_elements)\ncard_df = DataFrame({\n    'titles': card_data['title'],\n    'companies': card_data['company'],\n    'locations': card_data['location']\n})\ncard_df\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>titles</th>\n      <th>companies</th>\n      <th>locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Senior Python Developer</td>\n      <td>Payne, Roberts and Davis</td>\n      <td>Stewartbury, AA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Energy engineer</td>\n      <td>Vasquez-Davidson</td>\n      <td>Christopherville, AA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Legal executive</td>\n      <td>Jackson, Chambers and Levy</td>\n      <td>Port Ericaburgh, AA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fitness centre manager</td>\n      <td>Savage-Bradley</td>\n      <td>East Seanview, AP</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Product manager</td>\n      <td>Ramirez Inc</td>\n      <td>North Jamieview, AP</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Museum/gallery exhibitions officer</td>\n      <td>Nguyen, Yoder and Petty</td>\n      <td>Lake Abigail, AE</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>Radiographer, diagnostic</td>\n      <td>Holder LLC</td>\n      <td>Jacobshire, AP</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>Database administrator</td>\n      <td>Yates-Ferguson</td>\n      <td>Port Susan, AE</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>Furniture designer</td>\n      <td>Ortega-Lawrence</td>\n      <td>North Tiffany, AA</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>Ship broker</td>\n      <td>Fuentes, Walls and Castro</td>\n      <td>Michelleville, AP</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "web_scraping_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
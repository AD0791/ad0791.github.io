---
title: Web Scraping
subtitle: Scrape the web to retrieve your data
description: Python offers us multiple battle tested solutions to tackle this problem properly.
author: Alexandro Disla
author-meta: "Data Analyst & backend dev"
date: "2023-07-15"
date-modified: now
date-meta: time of the last article update
reading-time: 10
image: ../../img/web_scraping.jpeg
image-alt: The logo of web scraping
categories: [Python, "Web scraping"]
toc: true
toc-depth: 2
toc-expand: true

format:
  html:
    code-fold: true
jupyter: python3
---

# Web scraping

Web scraping is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.

If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own and make sure that you’re not violating any Terms of Service before you start a large-scale project.

## The need for Web scraping

Say you’re a surfer, both online and in real life, and you’re looking for employment. However, you’re not looking for just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way!

There’s a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You think about checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time.

Thankfully, the world offers other ways to apply that surfer’s mindset! Instead of looking at the job site every day, you can use Python to help automate your job search’s repetitive parts. Automated web scraping can be a solution to speed up the data collection process. You write your code once, and it will get the information you want many times and from many pages.

In contrast, when you try to get the information you want manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and repetition.

There’s so much information on the Web, and new information is constantly added. You’ll probably be interested in at least some of that data, and much of it is just out there for the taking. Whether you’re actually on the job hunt or you want to download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals.

## Available Options and Alternatives

7 Best Python Libraries For Web Scraping

### Quick comparison of the solutions

BeautifulSoup
Features of BeautifulSoup
Pros of BeautifulSoup
Cons of BeautifulSoup
Scrapy
Features of Scrapy
Pros of Scrapy
Cons of Scrapy
Selenium
Features of Selenium
Pros of Selenium
Cons of Selenium
Requests
Features of Requests
Pros of Requests
Cons of Requests
Urllib3
Features of urllib3
Pros of urllib3
Cons of urllib3
Lxml
Features of LXML
Pros of LXML
Cons of LXML
MechanicalSoup
Features of MechanicalSoup
Pros of MechanicalSoup
Cons of MechanicalSoup

### An Alternative to Web Scraping: APIs
Some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.

When you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.

The front-end presentation of a site might change often, but such a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data.

However, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality.

The approach and tools you need to gather information using APIs are outside the scope of this tutorial. To learn more about it, check out API Integration in Python.


> Now let's run a project with Beautiful Soup and scrapy

# Beautiful Soup

Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.

These instructions illustrate all major features of Beautiful Soup 4, with examples. I show you what the library is good for, how it works, how to use it, how to make it do what you want, and what to do when it violates your expectations.

This document covers Beautiful Soup version 4.12.1. The examples in this documentation were written for Python 3.8.

You might be looking for the documentation for Beautiful Soup 3. If so, you should know that Beautiful Soup 3 is no longer being developed and that all support for it was dropped on December 31, 2020. If you want to learn about the differences between Beautiful Soup 3 and Beautiful Soup 4, see Porting code to BS4.

# Scrapy

Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.

Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.

# testing area

write your code here

## BS4

```{python}
from requests import get
from bs4 import BeautifulSoup, ResultSet
from pandas import DataFrame
from typing import List,Dict, Any
```

The scraping process of fake-jobs website:

```{python}
url = "https://stackoverflow.com/questions"
response = get(url)

# Check if the request was successful
if response.status_code != 200:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")
    exit()

soup = BeautifulSoup(response.content, "html.parser")


# Just in case you need the html file
""" with open("stackoverflow_data.html",'wb') as file:
    file.write(
        soup.find('div',id="mainbar").find('div',class_="flush-left").prettify("utf-8")
    )
"""


questions = soup.select('.s-post-summary.js-post-summary')
```


```{python}
def questions_handler(questions: ResultSet):
    data={
        'title': [],
        'user': [],
        'vote_count': [],
        'answer_count': [],
        'view_count': []
    }
    for question in questions:
        title = question.select_one('.s-link').getText()
        user = question.select_one('.s-user-card--link a').getText()
        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()
        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()
        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()
        if title:
            data["title"].append(title)
        if user:
            data["user"].append(user)
        if vote_count:
            data["vote_count"].append(vote_count)
        if answer_count:
            data["answer_count"].append(answer_count)
        if view_count:
            data["view_count"].append(view_count)
    return data
```

```{python}
def build_dataframe(questions_set: ResultSet):
    data = questions_handler(questions=questions_set)
    if not data["title"]:
        return DataFrame({
            "message": ["The Stackoverflow page has been changed"],
            "action": ["correct the scraper"]
        })
    return DataFrame({
        "titles": data["title"],
        "users": data["user"],
        "vote_counts": data["vote_count"],
        "answer_counts": data["answer_count"],
        "view_counts": data["view_count"],
    })
    


```


Make a DataFrame:


```{python}
build_dataframe(questions)
```

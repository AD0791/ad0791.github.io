---
title: Web Scraping
subtitle: Scrape the web to retrieve your data
description: Python offers us multiple battle tested solutions to tackle this problem properly.
author: Alexandro Disla
author-meta: "Data Analyst & backend dev"
date: "2023-07-15"
date-modified: now
date-meta: time of the last article update
reading-time: 10
image: ../../img/web_scraping.jpeg
image-alt: The logo of web scraping
categories: [Python, "Web scraping"]
toc: true
toc-depth: 2
toc-expand: true

format:
  html:
    code-fold: true
jupyter: python3
---

# Web scraping: Extracting Data from Websites

Web scraping refers to techniques for collecting or extracting data from websites. It allows one to programmatically obtain large amounts of data that would be difficult or time-consuming to gather manually. It's very interesting to see how you can leverage your data analytical skills in this process. Let's see how everything will come together.

##  Why Web Scraping is Useful

Web scraping has many practical applications and uses. Here are some examples:

- Price monitoring - Track prices and price changes for products on e-commerce websites. This can help find deals or monitor trends.
- Lead generation - Gather contact information like emails and phone numbers from directories or listings. This is useful for sales and marketing.
- Research - Collect data from websites to perform analyses or conduct studies. Examples include gathering product reviews, compiling real estate listings, or analyzing social media trends.
- Content aggregation - Build databases or summaries by scraping news sites, blogs, classifieds, and other sources to create curated content sites.
- SEO monitoring - Check rankings and keyword positions for a site on search engines like Google. Helps optimize search marketing efforts.

## How Web Scraping works
Web scrapers access webpages programmatically and extract the desired information. Here is the general process:

1. Find the URL of the page to scrape. 

2. Send an HTTP request to download the page content.

3. Parse through the HTML content to identify relevant data. Common approaches include:

    - Pattern matching - Search for strings or regex patterns that identify data.

    - DOM parsing - Traverse the DOM (Document Object Model) tree to locate elements.

    - XPath queries - Write expressions to navigate through HTML structure and find data.

4. Extract and store the data, often in a database or spreadsheet.

5. Repeat the process across many pages to gather larger data sets.

Web scraping can be done through scripting languages like Python, libraries like BeautifulSoup, browser automation tools like Selenium, or fully integrated scraping solutions. 

### Key Considerations

There are a few key factors to keep in mind when web scraping:

- Avoid overloading websites with too many rapid requests, which can be seen as denial of service attacks. Add delays and throttles.

- Check websites' terms of use and robots.txt files to understand if they allow scraping. Some sites prohibit it.

- Use caches, proxies, and rotation to distribute requests and avoid getting IP addresses blocked.

- In some cases, explicitly identifying as a scraper through a user agent string can help avoid blocks. 

- Make sure to follow relevant laws and regulations regarding data collection and usage.

In summary, web scraping is a versatile technique to automate the extraction of data from websites for various purposes. When done properly, it is an extremely useful tool for data collection and analysis.

## Popular Python Web Scraping Frameworks

There are many Python libraries and frameworks that make web scraping easier. Some popular options include:

- BeautifulSoup - HTML/XML parsing library that helps navigate, search, and extract data from HTML. Excellent for basic scraping tasks.

- Scrapy - Full framework for large scale web crawling and scraping. Can extract data very quickly and handle large volumes. 

- Selenium - Automates web browsers to programmatically load pages and extract data. Useful when sites have heavy JavaScript or are harder to scrape.

- Requests - Simplifies making HTTP requests to access web pages. Good foundation for APIs and scraping.

- lxml - Fast and feature-rich library for XML and HTML manipulation. Helps scrape complex sites. 

These libraries can be combined to create powerful scrapers. For example, using Requests and BeautifulSoup together is a common approach. Scrapy and Selenium also integrate with BeautifulSoup.

## Analyzing Stack Overflow Question Data

Stack Overflow is one of the largest online communities for software developers to ask and answer programming questions. The site contains a wealth of data that can be analyzed to uncover interesting insights.

In this article, we will scrape a sample of recent Stack Overflow questions using Python and BeautifulSoup. We will then load the data into a Pandas DataFrame to analyze question statistics like tags, scores, views, etc.

### Scraping Stack Overflow Questions with BS4
We can use the BeautifulSoup library in Python to parse the HTML of the Stack Overflow homepage and extract the question data.

# testing area

write your code here

## BS4

```{python}
from requests import get
from bs4 import BeautifulSoup, ResultSet
from pandas import DataFrame
from typing import List,Dict, Any
```

The scraping process of fake-jobs website:

```{python}
url = "https://stackoverflow.com/questions"
response = get(url)

# Check if the request was successful
if response.status_code != 200:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")
    exit()

soup = BeautifulSoup(response.content, "html.parser")


# Just in case you need the html file
""" with open("stackoverflow_data.html",'wb') as file:
    file.write(
        soup.find('div',id="mainbar").find('div',class_="flush-left").prettify("utf-8")
    ) """

questions = soup.select('.s-post-summary.js-post-summary')
```


```{python}
def questions_handler(questions: ResultSet):
    data={
        'title': [],
        'user': [],
        'vote_count': [],
        'answer_count': [],
        'view_count': []
    }
    for question in questions:
        title = question.select_one('.s-link').getText()
        user = question.select_one('.s-user-card--link a').getText()
        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()
        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()
        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()
        if title:
            data["title"].append(title)
        if user:
            data["user"].append(user)
        if vote_count:
            data["vote_count"].append(vote_count)
        if answer_count:
            data["answer_count"].append(answer_count)
        if view_count:
            data["view_count"].append(view_count)
    return data
```

```{python}
def build_dataframe(questions_set: ResultSet):
    data = questions_handler(questions=questions_set)
    if not data["title"]:
        return DataFrame({
            "message": ["The Stackoverflow page has been changed"],
            "action": ["correct the scraper"]
        })
    return DataFrame({
        "titles": data["title"],
        "users": data["user"],
        "vote_counts": data["vote_count"],
        "answer_counts": data["answer_count"],
        "view_counts": data["view_count"],
    })
    


```


Make a DataFrame:


```{python}
build_dataframe(questions)
```

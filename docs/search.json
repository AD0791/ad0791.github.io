[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/projects/index.html",
    "href": "old_version/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-languages-of-middle-earth",
    "href": "old_version/projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "blog/posts/geoanalysis.html",
    "href": "blog/posts/geoanalysis.html",
    "title": "geoanalysis",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda_rs.html",
    "href": "blog/posts/eda_rs.html",
    "title": "The powers of the polars framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "The ingredients\n\n\n\nData Analysis & Visualization\n\n\nBackend Services\n\n\nScraping & Automation\n\n\n\n \n  \n   \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n  \n    \n     Upwork\n  \n\n\n\n\nWelcome to my website!\nI am a disciplined, detail-oriented person with good communication skills and appropriate training and work experience. My goal is to be highly efficient and deliver products that meet customer needs and requests.\nI have Graduated from a school of applied economics and statistics. I have multiples of experiences in the fields of data analysis, modeling and backend engineering. In my daily job, I do exploratory data analysis to ensure the data quality of the multiple sources of data within the organization.I submit the bad data to be corrected by the data entry team or the IT team. I build automation reports that met the specified requirements. These reports are a combination of sql (or mongo queries), sqlalchemy, pymongo, pandas (or polars) and tidyverse (if needed). The goal is to present the results in the required format (word, excell, powerpoint, visualization) or the required context (a python/R dashboard, a web services). I dabbled with wordpress to serve the needs of my clients.\nBelow is a detailed list of my skills.\n\nQuarto and Rmarkdown (an open-source scientific and technical publishing system)\nMicrosoft Excel, Jupyter Notebook, Polars & Pandas, Seaborn & GGplot\nGeopandas, folium and leaflet for GeoAnalysis\nWeb scraping with scrapy and beautifulSoup\nWeb automation with Selenium\nShiny, streamlit, dash for all in one dashboard solutions\nWordPress (website design and development)\nSQL scripts\nbackend development (fastapi, express, flask,Restapi, graphql, web socket)\n\nI’d like to make it clear that for me, it’s a priviledge to be here to tackle and solve your problems. It is much more than a way to earn money. It’s also my passion. So I’ll do my best in every detail of every order, because the customer’s ultimate satisfaction is my priority.\nPlease don’t hesitate to contact me and I’ll be delighted to work with you.\nThank you in advance for choosing me.\nYours sincerely\nAlexandro Disla\nPlease feel free to contact me if you have any questions or would like to discuss potential projects.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda.html",
    "href": "blog/posts/eda.html",
    "title": "The powers of the pandas framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/web_scraping.html",
    "href": "blog/posts/web_scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping refers to techniques for collecting or extracting data from websites. It allows one to programmatically obtain large amounts of data that would be difficult or time-consuming to gather manually. It’s very interesting to see how you can leverage your data analytical skills in this process. Let’s see how everything will come together.\n\n\nWeb scraping has many practical applications and uses. Here are some examples:\n\nPrice monitoring - Track prices and price changes for products on e-commerce websites. This can help find deals or monitor trends.\nLead generation - Gather contact information like emails and phone numbers from directories or listings. This is useful for sales and marketing.\nResearch - Collect data from websites to perform analyses or conduct studies. Examples include gathering product reviews, compiling real estate listings, or analyzing social media trends.\nContent aggregation - Build databases or summaries by scraping news sites, blogs, classifieds, and other sources to create curated content sites.\nSEO monitoring - Check rankings and keyword positions for a site on search engines like Google. Helps optimize search marketing efforts.\n\n\n\n\nWeb scrapers access webpages programmatically and extract the desired information. Here is the general process:\n\nFind the URL of the page to scrape.\nSend an HTTP request to download the page content.\nParse through the HTML content to identify relevant data. Common approaches include:\n\nPattern matching - Search for strings or regex patterns that identify data.\nDOM parsing - Traverse the DOM (Document Object Model) tree to locate elements.\nXPath queries - Write expressions to navigate through HTML structure and find data.\n\nExtract and store the data, often in a database or spreadsheet.\nRepeat the process across many pages to gather larger data sets.\n\nWeb scraping can be done through scripting languages like Python, libraries like BeautifulSoup, browser automation tools like Selenium, or fully integrated scraping solutions.\n\n\nThere are a few key factors to keep in mind when web scraping:\n\nAvoid overloading websites with too many rapid requests, which can be seen as denial of service attacks. Add delays and throttles.\nCheck websites’ terms of use and robots.txt files to understand if they allow scraping. Some sites prohibit it.\nUse caches, proxies, and rotation to distribute requests and avoid getting IP addresses blocked.\nIn some cases, explicitly identifying as a scraper through a user agent string can help avoid blocks.\nMake sure to follow relevant laws and regulations regarding data collection and usage.\n\nIn summary, web scraping is a versatile technique to automate the extraction of data from websites for various purposes. When done properly, it is an extremely useful tool for data collection and analysis.\n\n\n\n\nThere are many Python libraries and frameworks that make web scraping easier. Some popular options include:\n\nBeautifulSoup - HTML/XML parsing library that helps navigate, search, and extract data from HTML. Excellent for basic scraping tasks.\nScrapy - Full framework for large scale web crawling and scraping. Can extract data very quickly and handle large volumes.\nSelenium - Automates web browsers to programmatically load pages and extract data. Useful when sites have heavy JavaScript or are harder to scrape.\nRequests - Simplifies making HTTP requests to access web pages. Good foundation for APIs and scraping.\nlxml - Fast and feature-rich library for XML and HTML manipulation. Helps scrape complex sites.\n\nThese libraries can be combined to create powerful scrapers. For example, using Requests and BeautifulSoup together is a common approach. Scrapy and Selenium also integrate with BeautifulSoup.\n\n\n\nStack Overflow is one of the largest online communities for software developers to ask and answer programming questions. The site contains a wealth of data that can be analyzed to uncover interesting insights.\nIn this article, we will scrape a sample of recent Stack Overflow questions using Python and BeautifulSoup. We will then load the data into a Pandas DataFrame to analyze question statistics like views, answers, votes, etc.\n\n\nWe can use the BeautifulSoup library in Python to parse the HTML of the Stack Overflow homepage and extract the question data.\n\n\nCode\nfrom requests import get\nfrom bs4 import BeautifulSoup, ResultSet\nfrom pandas import DataFrame\nfrom typing import List,Dict, Any\nimport plotly.express as px\n\n\nThe scraping process of fake-jobs website:\n\n\nCode\nurl = \"https://stackoverflow.com/questions\"\nresponse = get(url)\n\n# Check if the request was successful\nif response.status_code != 200:\n    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n    exit()\n\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n\n# Just in case you need the html file\n\"\"\" with open(\"stackoverflow_data.html\",'wb') as file:\n    file.write(\n        soup.find('div',id=\"mainbar\").find('div',class_=\"flush-left\").prettify(\"utf-8\")\n    ) \"\"\"\n\nquestions = soup.select('.s-post-summary.js-post-summary')\n\n\n\n\nCode\ndef questions_handler(questions: ResultSet):\n    data={\n        'title': [],\n        'user': [],\n        'vote_count': [],\n        'answer_count': [],\n        'view_count': []\n    }\n    for question in questions:\n        title = question.select_one('.s-link').getText()\n        user = question.select_one('.s-user-card--link a').getText()\n        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()\n        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()\n        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()\n        if title:\n            data[\"title\"].append(title)\n        if user:\n            data[\"user\"].append(user)\n        if vote_count:\n            data[\"vote_count\"].append(vote_count)\n        if answer_count:\n            data[\"answer_count\"].append(answer_count)\n        if view_count:\n            data[\"view_count\"].append(view_count)\n    return data\n\n\n\n\nCode\ndef build_dataframe(questions_set: ResultSet):\n    data = questions_handler(questions=questions_set)\n    if not data[\"title\"]:\n        return DataFrame({\n            \"message\": [\"The Stackoverflow page has been changed\"],\n            \"action\": [\"correct the scraper\"]\n        })\n    return DataFrame({\n        \"titles\": data[\"title\"],\n        \"users\": data[\"user\"],\n        \"vote_counts\": data[\"vote_count\"],\n        \"answer_counts\": data[\"answer_count\"],\n        \"view_counts\": data[\"view_count\"],\n    })\n\n\n\n\n\n\nCode\nquestions_df = build_dataframe(questions)\nquestions_df.head(1)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n\n\n\n\n\n\n\n\n\nremove the newline who is messing with the counts strings\n\n\n\nCode\nquestions_df['vote_counts'] = questions_df['vote_counts'].str.replace('\\n', ' ')\nquestions_df['answer_counts'] = questions_df['answer_counts'].str.replace('\\n', ' ')\nquestions_df['view_counts'] = questions_df['view_counts'].str.replace('\\n', ' ')\n\nquestions_df.head(1)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n0 votes\n0 answers\n2 views\n\n\n\n\n\n\n\n\nExtract the views, answers and the votes\n\n\n\nCode\nquestions_df['vote_counts'] = questions_df['vote_counts'].str.extract('(\\d+)').astype(int)\nquestions_df['answer_counts'] = questions_df['answer_counts'].str.extract('(\\d+)').astype(int)\nquestions_df['view_counts'] = questions_df['view_counts'].str.extract('(\\d+)').astype(int)\nquestions_df.rename(columns={\n    \"vote_counts\": \"votes\",\n    \"answer_counts\": \"answers\",\n    \"view_counts\": \"views\"\n},\ninplace=True\n)\nquestions_df.head(5)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvotes\nanswers\nviews\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n0\n0\n2\n\n\n1\nHow do you double every other element within a...\ndarryl\n0\n0\n2\n\n\n2\nIs it possible to accelerate data copying betw...\nE_K\n0\n0\n2\n\n\n3\nAvoiding Password Leaks\nI am not lazy\n0\n0\n3\n\n\n4\nformatting pod names with bash shell\nThink Different\n0\n0\n2\n\n\n\n\n\n\n\n\nWe can choose to save it to excel format, produce plots or pivot_tables.\n\n\n\nThe views by Users:\n\n\nCode\npx.bar(questions_df, x=\"views\", y=\"users\", text_auto=True)\n\n\n\n                                                \n\n\nThe answers by Users:\n\n\nCode\npx.bar(questions_df, x=\"answers\", y=\"users\", text_auto=True)\n\n\n\n                                                \n\n\nThe votes by Users:\n\n\nCode\npx.bar(questions_df, x=\"votes\", y=\"users\")\n\n\n\n                                                \n\n\n\n\n\n\n\nCode\nquestions_df.pivot_table(\n    values=[\"views\",\"answers\",\"votes\"],\n    index = 'users',\n    aggfunc='sum',\n    fill_value=0,\n    margins=True,\n    margins_name=\"Total\"\n)\n\n\n\n\n\n\n\n\n\nanswers\nviews\nvotes\n\n\nusers\n\n\n\n\n\n\n\nASHISH M.G\n0\n3\n0\n\n\nBajrang Singh\n0\n5\n0\n\n\nBruno\n0\n3\n0\n\n\nDBtake3\n0\n4\n0\n\n\nDoveman\n0\n2\n0\n\n\nE_K\n0\n2\n0\n\n\nEsteban Rodofili\n0\n3\n0\n\n\nI am not lazy\n0\n3\n0\n\n\nIbrahim Farooq\n0\n4\n0\n\n\nJordash\n0\n3\n0\n\n\nJoy\n0\n6\n0\n\n\nJoy Gupta\n0\n6\n0\n\n\nJustin\n0\n4\n0\n\n\nKingly Lee\n0\n3\n0\n\n\nKrishnaveni B\n0\n5\n0\n\n\nLincoln Emilio Bowen Aguayo\n0\n4\n0\n\n\nLucas\n0\n5\n0\n\n\nLucas Batista\n0\n10\n0\n\n\nMayank Gautam\n0\n6\n2\n\n\nMeh Mech\n0\n9\n0\n\n\nMiRAY\n0\n7\n0\n\n\nMorphois\n0\n6\n0\n\n\nMoshe\n0\n4\n0\n\n\nNcls7523\n1\n7\n0\n\n\nNunyet de Can Calçada\n0\n6\n0\n\n\nOthmen Khchimi\n0\n3\n0\n\n\nOutstretched Pupil\n0\n7\n0\n\n\nQuantum Phantasy\n0\n7\n2\n\n\nRGS\n0\n3\n0\n\n\nSiyum\n0\n5\n0\n\n\nSyed Zeerak Hussain Gillani\n0\n4\n0\n\n\nThanos Rom\n0\n4\n0\n\n\nThesselia's\n0\n3\n0\n\n\nThink Different\n0\n2\n0\n\n\nTyler Weaver\n0\n3\n0\n\n\nVishal Patel\n0\n4\n0\n\n\nZachary Fearnside\n0\n6\n2\n\n\nbogdanel2011 Sandu\n0\n2\n0\n\n\ndarryl\n0\n2\n0\n\n\ndinesh\n0\n3\n0\n\n\nlinkho\n0\n3\n0\n\n\nmohammad shafi\n0\n7\n0\n\n\nmoosecles\n0\n8\n0\n\n\nquadron21\n0\n9\n0\n\n\nthemujahidkhan\n0\n3\n0\n\n\ntwelfth\n1\n8\n0\n\n\nuser2291499\n0\n4\n0\n\n\nvanoski\n0\n6\n0\n\n\nxD_EL\n0\n4\n0\n\n\n卢明达\n0\n11\n1\n\n\nTotal\n2\n241\n7\n\n\n\n\n\n\n\n\n\n\n\n\nScrapy is a popular web scraping framework for Python. It can be used to build scrapers that efficiently crawl websites and extract data.\n\n\nFirst, we need to create a Spider class that will crawl the Stack Overflow URL and parse the response:"
  },
  {
    "objectID": "blog/posts/web_scraping.html#why-web-scraping-is-useful",
    "href": "blog/posts/web_scraping.html#why-web-scraping-is-useful",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping has many practical applications and uses. Here are some examples:\n\nPrice monitoring - Track prices and price changes for products on e-commerce websites. This can help find deals or monitor trends.\nLead generation - Gather contact information like emails and phone numbers from directories or listings. This is useful for sales and marketing.\nResearch - Collect data from websites to perform analyses or conduct studies. Examples include gathering product reviews, compiling real estate listings, or analyzing social media trends.\nContent aggregation - Build databases or summaries by scraping news sites, blogs, classifieds, and other sources to create curated content sites.\nSEO monitoring - Check rankings and keyword positions for a site on search engines like Google. Helps optimize search marketing efforts."
  },
  {
    "objectID": "blog/posts/web_scraping.html#how-web-scraping-works",
    "href": "blog/posts/web_scraping.html#how-web-scraping-works",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scrapers access webpages programmatically and extract the desired information. Here is the general process:\n\nFind the URL of the page to scrape.\nSend an HTTP request to download the page content.\nParse through the HTML content to identify relevant data. Common approaches include:\n\nPattern matching - Search for strings or regex patterns that identify data.\nDOM parsing - Traverse the DOM (Document Object Model) tree to locate elements.\nXPath queries - Write expressions to navigate through HTML structure and find data.\n\nExtract and store the data, often in a database or spreadsheet.\nRepeat the process across many pages to gather larger data sets.\n\nWeb scraping can be done through scripting languages like Python, libraries like BeautifulSoup, browser automation tools like Selenium, or fully integrated scraping solutions.\n\n\nThere are a few key factors to keep in mind when web scraping:\n\nAvoid overloading websites with too many rapid requests, which can be seen as denial of service attacks. Add delays and throttles.\nCheck websites’ terms of use and robots.txt files to understand if they allow scraping. Some sites prohibit it.\nUse caches, proxies, and rotation to distribute requests and avoid getting IP addresses blocked.\nIn some cases, explicitly identifying as a scraper through a user agent string can help avoid blocks.\nMake sure to follow relevant laws and regulations regarding data collection and usage.\n\nIn summary, web scraping is a versatile technique to automate the extraction of data from websites for various purposes. When done properly, it is an extremely useful tool for data collection and analysis."
  },
  {
    "objectID": "blog/posts/web_scraping.html#popular-python-web-scraping-frameworks",
    "href": "blog/posts/web_scraping.html#popular-python-web-scraping-frameworks",
    "title": "Web Scraping",
    "section": "",
    "text": "There are many Python libraries and frameworks that make web scraping easier. Some popular options include:\n\nBeautifulSoup - HTML/XML parsing library that helps navigate, search, and extract data from HTML. Excellent for basic scraping tasks.\nScrapy - Full framework for large scale web crawling and scraping. Can extract data very quickly and handle large volumes.\nSelenium - Automates web browsers to programmatically load pages and extract data. Useful when sites have heavy JavaScript or are harder to scrape.\nRequests - Simplifies making HTTP requests to access web pages. Good foundation for APIs and scraping.\nlxml - Fast and feature-rich library for XML and HTML manipulation. Helps scrape complex sites.\n\nThese libraries can be combined to create powerful scrapers. For example, using Requests and BeautifulSoup together is a common approach. Scrapy and Selenium also integrate with BeautifulSoup."
  },
  {
    "objectID": "blog/posts/web_scraping.html#analyzing-stack-overflow-question-data",
    "href": "blog/posts/web_scraping.html#analyzing-stack-overflow-question-data",
    "title": "Web Scraping",
    "section": "",
    "text": "Stack Overflow is one of the largest online communities for software developers to ask and answer programming questions. The site contains a wealth of data that can be analyzed to uncover interesting insights.\nIn this article, we will scrape a sample of recent Stack Overflow questions using Python and BeautifulSoup. We will then load the data into a Pandas DataFrame to analyze question statistics like views, answers, votes, etc.\n\n\nWe can use the BeautifulSoup library in Python to parse the HTML of the Stack Overflow homepage and extract the question data.\n\n\nCode\nfrom requests import get\nfrom bs4 import BeautifulSoup, ResultSet\nfrom pandas import DataFrame\nfrom typing import List,Dict, Any\nimport plotly.express as px\n\n\nThe scraping process of fake-jobs website:\n\n\nCode\nurl = \"https://stackoverflow.com/questions\"\nresponse = get(url)\n\n# Check if the request was successful\nif response.status_code != 200:\n    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n    exit()\n\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n\n# Just in case you need the html file\n\"\"\" with open(\"stackoverflow_data.html\",'wb') as file:\n    file.write(\n        soup.find('div',id=\"mainbar\").find('div',class_=\"flush-left\").prettify(\"utf-8\")\n    ) \"\"\"\n\nquestions = soup.select('.s-post-summary.js-post-summary')\n\n\n\n\nCode\ndef questions_handler(questions: ResultSet):\n    data={\n        'title': [],\n        'user': [],\n        'vote_count': [],\n        'answer_count': [],\n        'view_count': []\n    }\n    for question in questions:\n        title = question.select_one('.s-link').getText()\n        user = question.select_one('.s-user-card--link a').getText()\n        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()\n        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()\n        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()\n        if title:\n            data[\"title\"].append(title)\n        if user:\n            data[\"user\"].append(user)\n        if vote_count:\n            data[\"vote_count\"].append(vote_count)\n        if answer_count:\n            data[\"answer_count\"].append(answer_count)\n        if view_count:\n            data[\"view_count\"].append(view_count)\n    return data\n\n\n\n\nCode\ndef build_dataframe(questions_set: ResultSet):\n    data = questions_handler(questions=questions_set)\n    if not data[\"title\"]:\n        return DataFrame({\n            \"message\": [\"The Stackoverflow page has been changed\"],\n            \"action\": [\"correct the scraper\"]\n        })\n    return DataFrame({\n        \"titles\": data[\"title\"],\n        \"users\": data[\"user\"],\n        \"vote_counts\": data[\"vote_count\"],\n        \"answer_counts\": data[\"answer_count\"],\n        \"view_counts\": data[\"view_count\"],\n    })\n\n\n\n\n\n\nCode\nquestions_df = build_dataframe(questions)\nquestions_df.head(1)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n\n\n\n\n\n\n\n\n\nremove the newline who is messing with the counts strings\n\n\n\nCode\nquestions_df['vote_counts'] = questions_df['vote_counts'].str.replace('\\n', ' ')\nquestions_df['answer_counts'] = questions_df['answer_counts'].str.replace('\\n', ' ')\nquestions_df['view_counts'] = questions_df['view_counts'].str.replace('\\n', ' ')\n\nquestions_df.head(1)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n0 votes\n0 answers\n2 views\n\n\n\n\n\n\n\n\nExtract the views, answers and the votes\n\n\n\nCode\nquestions_df['vote_counts'] = questions_df['vote_counts'].str.extract('(\\d+)').astype(int)\nquestions_df['answer_counts'] = questions_df['answer_counts'].str.extract('(\\d+)').astype(int)\nquestions_df['view_counts'] = questions_df['view_counts'].str.extract('(\\d+)').astype(int)\nquestions_df.rename(columns={\n    \"vote_counts\": \"votes\",\n    \"answer_counts\": \"answers\",\n    \"view_counts\": \"views\"\n},\ninplace=True\n)\nquestions_df.head(5)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvotes\nanswers\nviews\n\n\n\n\n0\nI tried making a turbowarp extension, but i ca...\nbogdanel2011 Sandu\n0\n0\n2\n\n\n1\nHow do you double every other element within a...\ndarryl\n0\n0\n2\n\n\n2\nIs it possible to accelerate data copying betw...\nE_K\n0\n0\n2\n\n\n3\nAvoiding Password Leaks\nI am not lazy\n0\n0\n3\n\n\n4\nformatting pod names with bash shell\nThink Different\n0\n0\n2\n\n\n\n\n\n\n\n\nWe can choose to save it to excel format, produce plots or pivot_tables.\n\n\n\nThe views by Users:\n\n\nCode\npx.bar(questions_df, x=\"views\", y=\"users\", text_auto=True)\n\n\n\n                                                \n\n\nThe answers by Users:\n\n\nCode\npx.bar(questions_df, x=\"answers\", y=\"users\", text_auto=True)\n\n\n\n                                                \n\n\nThe votes by Users:\n\n\nCode\npx.bar(questions_df, x=\"votes\", y=\"users\")\n\n\n\n                                                \n\n\n\n\n\n\n\nCode\nquestions_df.pivot_table(\n    values=[\"views\",\"answers\",\"votes\"],\n    index = 'users',\n    aggfunc='sum',\n    fill_value=0,\n    margins=True,\n    margins_name=\"Total\"\n)\n\n\n\n\n\n\n\n\n\nanswers\nviews\nvotes\n\n\nusers\n\n\n\n\n\n\n\nASHISH M.G\n0\n3\n0\n\n\nBajrang Singh\n0\n5\n0\n\n\nBruno\n0\n3\n0\n\n\nDBtake3\n0\n4\n0\n\n\nDoveman\n0\n2\n0\n\n\nE_K\n0\n2\n0\n\n\nEsteban Rodofili\n0\n3\n0\n\n\nI am not lazy\n0\n3\n0\n\n\nIbrahim Farooq\n0\n4\n0\n\n\nJordash\n0\n3\n0\n\n\nJoy\n0\n6\n0\n\n\nJoy Gupta\n0\n6\n0\n\n\nJustin\n0\n4\n0\n\n\nKingly Lee\n0\n3\n0\n\n\nKrishnaveni B\n0\n5\n0\n\n\nLincoln Emilio Bowen Aguayo\n0\n4\n0\n\n\nLucas\n0\n5\n0\n\n\nLucas Batista\n0\n10\n0\n\n\nMayank Gautam\n0\n6\n2\n\n\nMeh Mech\n0\n9\n0\n\n\nMiRAY\n0\n7\n0\n\n\nMorphois\n0\n6\n0\n\n\nMoshe\n0\n4\n0\n\n\nNcls7523\n1\n7\n0\n\n\nNunyet de Can Calçada\n0\n6\n0\n\n\nOthmen Khchimi\n0\n3\n0\n\n\nOutstretched Pupil\n0\n7\n0\n\n\nQuantum Phantasy\n0\n7\n2\n\n\nRGS\n0\n3\n0\n\n\nSiyum\n0\n5\n0\n\n\nSyed Zeerak Hussain Gillani\n0\n4\n0\n\n\nThanos Rom\n0\n4\n0\n\n\nThesselia's\n0\n3\n0\n\n\nThink Different\n0\n2\n0\n\n\nTyler Weaver\n0\n3\n0\n\n\nVishal Patel\n0\n4\n0\n\n\nZachary Fearnside\n0\n6\n2\n\n\nbogdanel2011 Sandu\n0\n2\n0\n\n\ndarryl\n0\n2\n0\n\n\ndinesh\n0\n3\n0\n\n\nlinkho\n0\n3\n0\n\n\nmohammad shafi\n0\n7\n0\n\n\nmoosecles\n0\n8\n0\n\n\nquadron21\n0\n9\n0\n\n\nthemujahidkhan\n0\n3\n0\n\n\ntwelfth\n1\n8\n0\n\n\nuser2291499\n0\n4\n0\n\n\nvanoski\n0\n6\n0\n\n\nxD_EL\n0\n4\n0\n\n\n卢明达\n0\n11\n1\n\n\nTotal\n2\n241\n7\n\n\n\n\n\n\n\n\n\n\n\n\nScrapy is a popular web scraping framework for Python. It can be used to build scrapers that efficiently crawl websites and extract data.\n\n\nFirst, we need to create a Spider class that will crawl the Stack Overflow URL and parse the response:"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Learn with me",
    "section": "",
    "text": "This will brings you value:\n\n\n   \n     \n     Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\nPython\n\n\nWeb scraping\n\n\n\nPython offers us multiple battle tested solutions to tackle this problem properly.\n\n\n\nAlexandro Disla\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the pandas framework\n\n\n\nPython\n\n\nPandas\n\n\nEDA\n\n\n\nStandard framework for data wrangling and analysis\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the polars framework\n\n\n\nPython\n\n\nPolars\n\n\nEDA\n\n\n\nRobust framework for data wrangling compatible with pandas and any data visualization libraries\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeoanalysis\n\n\n\nPython\n\n\nR\n\n\nGeoAnalysis\n\n\n\nWe have multiple solutions available with R and Python. Let’s explore them.\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n  Don't forget to take a good cup of cofee\n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/photography/index.html",
    "href": "old_version/photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)\n\n\n\n\n Back to top"
  }
]
[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/projects/index.html",
    "href": "old_version/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-languages-of-middle-earth",
    "href": "old_version/projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "blog/posts/geoanalysis.html",
    "href": "blog/posts/geoanalysis.html",
    "title": "geoanalysis",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda_rs.html",
    "href": "blog/posts/eda_rs.html",
    "title": "The powers of the polars framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "The ingredients\n\n\n\nData Analysis & Visualization\n\n\nBackend Services\n\n\nScraping & Automation\n\n\n\n \n  \n   \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n  \n    \n     Upwork\n  \n\n\n\n\nWelcome to my website!\nI am a disciplined, detail-oriented person with good communication skills and appropriate training and work experience. My goal is to be highly efficient and deliver products that meet customer needs and requests.\nI have Graduated from a school of applied economics and statistics. I have multiples of experiences in the fields of data analysis, modeling and backend engineering. In my daily job, I do exploratory data analysis to ensure the data quality of the multiple sources of data within the organization.I submit the bad data to be corrected by the data entry team or the IT team. I build automation reports that met the specified requirements. These reports are a combination of sql (or mongo queries), sqlalchemy, pymongo, pandas (or polars) and tidyverse (if needed). The goal is to present the results in the required format (word, excell, powerpoint, visualization) or the required context (a python/R dashboard, a web services). I dabbled with wordpress to serve the needs of my clients.\nBelow is a detailed list of my skills.\n\nQuarto and Rmarkdown (an open-source scientific and technical publishing system)\nMicrosoft Excel, Jupyter Notebook, Polars & Pandas, Seaborn & GGplot\nGeopandas, folium and leaflet for GeoAnalysis\nWeb scraping with scrapy and beautifulSoup\nWeb automation with Selenium\nShiny, streamlit, dash for all in one dashboard solutions\nWordPress (website design and development)\nSQL scripts\nbackend development (fastapi, express, flask,Restapi, graphql, web socket)\n\nI’d like to make it clear that for me, it’s a priviledge to be here to tackle and solve your problems. It is much more than a way to earn money. It’s also my passion. So I’ll do my best in every detail of every order, because the customer’s ultimate satisfaction is my priority.\nPlease don’t hesitate to contact me and I’ll be delighted to work with you.\nThank you in advance for choosing me.\nYours sincerely\nAlexandro Disla\nPlease feel free to contact me if you have any questions or would like to discuss potential projects.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda.html",
    "href": "blog/posts/eda.html",
    "title": "The powers of the pandas framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/web_scraping.html",
    "href": "blog/posts/web_scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping refers to techniques for collecting or extracting data from websites. It allows one to programmatically obtain large amounts of data that would be difficult or time-consuming to gather manually. It’s very interesting to see how you can leverage your data analytical skills in this process. Let’s see how everything will come together.\n\n\nWeb scraping has many practical applications and uses. Here are some examples:\n\nPrice monitoring - Track prices and price changes for products on e-commerce websites. This can help find deals or monitor trends.\nLead generation - Gather contact information like emails and phone numbers from directories or listings. This is useful for sales and marketing.\nResearch - Collect data from websites to perform analyses or conduct studies. Examples include gathering product reviews, compiling real estate listings, or analyzing social media trends.\nContent aggregation - Build databases or summaries by scraping news sites, blogs, classifieds, and other sources to create curated content sites.\nSEO monitoring - Check rankings and keyword positions for a site on search engines like Google. Helps optimize search marketing efforts.\n\n\n\n\nWeb scrapers access webpages programmatically and extract the desired information. Here is the general process:\n\nFind the URL of the page to scrape.\nSend an HTTP request to download the page content.\nParse through the HTML content to identify relevant data. Common approaches include:\n\nPattern matching - Search for strings or regex patterns that identify data.\nDOM parsing - Traverse the DOM (Document Object Model) tree to locate elements.\nXPath queries - Write expressions to navigate through HTML structure and find data.\n\nExtract and store the data, often in a database or spreadsheet.\nRepeat the process across many pages to gather larger data sets.\n\nWeb scraping can be done through scripting languages like Python, libraries like BeautifulSoup, browser automation tools like Selenium, or fully integrated scraping solutions.\n\n\nThere are a few key factors to keep in mind when web scraping:\n\nAvoid overloading websites with too many rapid requests, which can be seen as denial of service attacks. Add delays and throttles.\nCheck websites’ terms of use and robots.txt files to understand if they allow scraping. Some sites prohibit it.\nUse caches, proxies, and rotation to distribute requests and avoid getting IP addresses blocked.\nIn some cases, explicitly identifying as a scraper through a user agent string can help avoid blocks.\nMake sure to follow relevant laws and regulations regarding data collection and usage.\n\nIn summary, web scraping is a versatile technique to automate the extraction of data from websites for various purposes. When done properly, it is an extremely useful tool for data collection and analysis.\n\n\n\n\nThere are many Python libraries and frameworks that make web scraping easier. Some popular options include:\n\nBeautifulSoup - HTML/XML parsing library that helps navigate, search, and extract data from HTML. Excellent for basic scraping tasks.\nScrapy - Full framework for large scale web crawling and scraping. Can extract data very quickly and handle large volumes.\nSelenium - Automates web browsers to programmatically load pages and extract data. Useful when sites have heavy JavaScript or are harder to scrape.\nRequests - Simplifies making HTTP requests to access web pages. Good foundation for APIs and scraping.\nlxml - Fast and feature-rich library for XML and HTML manipulation. Helps scrape complex sites.\n\nThese libraries can be combined to create powerful scrapers. For example, using Requests and BeautifulSoup together is a common approach. Scrapy and Selenium also integrate with BeautifulSoup.\n\n\n\nStack Overflow is one of the largest online communities for software developers to ask and answer programming questions. The site contains a wealth of data that can be analyzed to uncover interesting insights.\nIn this article, we will scrape a sample of recent Stack Overflow questions using Python and BeautifulSoup. We will then load the data into a Pandas DataFrame to analyze question statistics like tags, scores, views, etc.\n\n\nWe can use the BeautifulSoup library in Python to parse the HTML of the Stack Overflow homepage and extract the question data."
  },
  {
    "objectID": "blog/posts/web_scraping.html#why-web-scraping-is-useful",
    "href": "blog/posts/web_scraping.html#why-web-scraping-is-useful",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping has many practical applications and uses. Here are some examples:\n\nPrice monitoring - Track prices and price changes for products on e-commerce websites. This can help find deals or monitor trends.\nLead generation - Gather contact information like emails and phone numbers from directories or listings. This is useful for sales and marketing.\nResearch - Collect data from websites to perform analyses or conduct studies. Examples include gathering product reviews, compiling real estate listings, or analyzing social media trends.\nContent aggregation - Build databases or summaries by scraping news sites, blogs, classifieds, and other sources to create curated content sites.\nSEO monitoring - Check rankings and keyword positions for a site on search engines like Google. Helps optimize search marketing efforts."
  },
  {
    "objectID": "blog/posts/web_scraping.html#how-web-scraping-works",
    "href": "blog/posts/web_scraping.html#how-web-scraping-works",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scrapers access webpages programmatically and extract the desired information. Here is the general process:\n\nFind the URL of the page to scrape.\nSend an HTTP request to download the page content.\nParse through the HTML content to identify relevant data. Common approaches include:\n\nPattern matching - Search for strings or regex patterns that identify data.\nDOM parsing - Traverse the DOM (Document Object Model) tree to locate elements.\nXPath queries - Write expressions to navigate through HTML structure and find data.\n\nExtract and store the data, often in a database or spreadsheet.\nRepeat the process across many pages to gather larger data sets.\n\nWeb scraping can be done through scripting languages like Python, libraries like BeautifulSoup, browser automation tools like Selenium, or fully integrated scraping solutions.\n\n\nThere are a few key factors to keep in mind when web scraping:\n\nAvoid overloading websites with too many rapid requests, which can be seen as denial of service attacks. Add delays and throttles.\nCheck websites’ terms of use and robots.txt files to understand if they allow scraping. Some sites prohibit it.\nUse caches, proxies, and rotation to distribute requests and avoid getting IP addresses blocked.\nIn some cases, explicitly identifying as a scraper through a user agent string can help avoid blocks.\nMake sure to follow relevant laws and regulations regarding data collection and usage.\n\nIn summary, web scraping is a versatile technique to automate the extraction of data from websites for various purposes. When done properly, it is an extremely useful tool for data collection and analysis."
  },
  {
    "objectID": "blog/posts/web_scraping.html#popular-python-web-scraping-frameworks",
    "href": "blog/posts/web_scraping.html#popular-python-web-scraping-frameworks",
    "title": "Web Scraping",
    "section": "",
    "text": "There are many Python libraries and frameworks that make web scraping easier. Some popular options include:\n\nBeautifulSoup - HTML/XML parsing library that helps navigate, search, and extract data from HTML. Excellent for basic scraping tasks.\nScrapy - Full framework for large scale web crawling and scraping. Can extract data very quickly and handle large volumes.\nSelenium - Automates web browsers to programmatically load pages and extract data. Useful when sites have heavy JavaScript or are harder to scrape.\nRequests - Simplifies making HTTP requests to access web pages. Good foundation for APIs and scraping.\nlxml - Fast and feature-rich library for XML and HTML manipulation. Helps scrape complex sites.\n\nThese libraries can be combined to create powerful scrapers. For example, using Requests and BeautifulSoup together is a common approach. Scrapy and Selenium also integrate with BeautifulSoup."
  },
  {
    "objectID": "blog/posts/web_scraping.html#analyzing-stack-overflow-question-data",
    "href": "blog/posts/web_scraping.html#analyzing-stack-overflow-question-data",
    "title": "Web Scraping",
    "section": "",
    "text": "Stack Overflow is one of the largest online communities for software developers to ask and answer programming questions. The site contains a wealth of data that can be analyzed to uncover interesting insights.\nIn this article, we will scrape a sample of recent Stack Overflow questions using Python and BeautifulSoup. We will then load the data into a Pandas DataFrame to analyze question statistics like tags, scores, views, etc.\n\n\nWe can use the BeautifulSoup library in Python to parse the HTML of the Stack Overflow homepage and extract the question data."
  },
  {
    "objectID": "blog/posts/web_scraping.html#bs4",
    "href": "blog/posts/web_scraping.html#bs4",
    "title": "Web Scraping",
    "section": "BS4",
    "text": "BS4\n\n\nCode\nfrom requests import get\nfrom bs4 import BeautifulSoup, ResultSet\nfrom pandas import DataFrame\nfrom typing import List,Dict, Any\n\n\nThe scraping process of fake-jobs website:\n\n\nCode\nurl = \"https://stackoverflow.com/questions\"\nresponse = get(url)\n\n# Check if the request was successful\nif response.status_code != 200:\n    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n    exit()\n\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n\n# Just in case you need the html file\n\"\"\" with open(\"stackoverflow_data.html\",'wb') as file:\n    file.write(\n        soup.find('div',id=\"mainbar\").find('div',class_=\"flush-left\").prettify(\"utf-8\")\n    ) \"\"\"\n\nquestions = soup.select('.s-post-summary.js-post-summary')\n\n\n\n\nCode\ndef questions_handler(questions: ResultSet):\n    data={\n        'title': [],\n        'user': [],\n        'vote_count': [],\n        'answer_count': [],\n        'view_count': []\n    }\n    for question in questions:\n        title = question.select_one('.s-link').getText()\n        user = question.select_one('.s-user-card--link a').getText()\n        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()\n        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()\n        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()\n        if title:\n            data[\"title\"].append(title)\n        if user:\n            data[\"user\"].append(user)\n        if vote_count:\n            data[\"vote_count\"].append(vote_count)\n        if answer_count:\n            data[\"answer_count\"].append(answer_count)\n        if view_count:\n            data[\"view_count\"].append(view_count)\n    return data\n\n\n\n\nCode\ndef build_dataframe(questions_set: ResultSet):\n    data = questions_handler(questions=questions_set)\n    if not data[\"title\"]:\n        return DataFrame({\n            \"message\": [\"The Stackoverflow page has been changed\"],\n            \"action\": [\"correct the scraper\"]\n        })\n    return DataFrame({\n        \"titles\": data[\"title\"],\n        \"users\": data[\"user\"],\n        \"vote_counts\": data[\"vote_count\"],\n        \"answer_counts\": data[\"answer_count\"],\n        \"view_counts\": data[\"view_count\"],\n    })\n\n\nMake a DataFrame:\n\n\nCode\nbuild_dataframe(questions)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nHow I can deploy superset on K8s using helm ch...\nRaymundo Hernández\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n1\nHow Would I fix receiving and invalid token er...\nPeyton S.\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n2\nHow to scale and optimize this friend suggesti...\nBear Bile Farming is Torture\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n3\nAppending to a TextField when the user has fin...\nA_toaster\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n4\nReact Native Hermes \"dlopen failed: library \"l...\nN. Doe\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n5\nc++20 Check a String at compile time for subst...\nJHeni\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n6\nR Roc curve small sample with random effect wi...\nNorpantytär\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n7\nget_template_part() not working when trying to...\nChris\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n8\nChrome Extension chrome.action.onClick breakin...\nPatrick Conrad\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n9\nPost Tweet using BirdElephant\nCarol.Kar\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n10\nSQL aggregation with GROUP_BY where in custom ...\nOleksiiGa\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n11\nComo resolver o problema da cobrinha não cresc...\nBrenda Montenegro\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n12\nObject reference not set to an instance of an ...\nJeffrey Davis\n\\n-2\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n13\nEstimating parameters of multiple weibull dist...\nsakar299\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n14\nAmphp parallel adding lots of amp-parallel soc...\nMindGamer\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n15\nIssues with changing state of each item in a l...\nZimpy\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n16\nMake slash command node.js discord bot\nblinkingcape\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n17\nLaravel and multiple tables joined by one column\nLostB\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n18\n&lt;a&gt; that does not have an href, cannot click o...\nLawnDart\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n11\\nviews\\n\n\n\n19\nSyntaxError: Cannot use import statement outsi...\nsadasdadssad\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n20\nGroup by and Order by detailed execution in JP...\nMayank Chauhan\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n21\nOpenCL multiple indices reduction\nFrobeniusnorm\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n22\nIssue when inserting content into DOM element ...\nJessica\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n12\\nviews\\n\n\n\n23\nCan't run acceptance test because com/intuit/k...\nCole Gvozdas\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n24\nHow to code =MATCH(1,(A:A=J1)*(B:B=K1)*(C:C=L1...\nuser22290834\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n10\\nviews\\n\n\n\n25\nHow to create docker image with java 17 [closed]\nuser1666649\n\\n0\\nvotes\\n\n\\n1\\nanswer\\n\n\\n14\\nviews\\n\n\n\n26\nWhat's the relevance of make-register acceptin...\nEnlico\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n27\nCould not chdir to home directory /home/cinnam...\nCinnamonTwirls\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n28\nResolve circular Unit Reference\nWolfgang Bures\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n29\nUnit tests fail on MacBook with AppleSilicon M1\nDJ-Glock\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n30\nError in \"npm i\" Command Running To Install No...\nsaima ali\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n8\\nviews\\n\n\n\n31\nWorkfront: peer review / edits of custom forms\nSteve Parmelee\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n32\nHow to match a regex expression only if a word...\ntezzaaa\n\\n0\\nvotes\\n\n\\n2\\nanswers\\n\n\\n16\\nviews\\n\n\n\n33\nNuxt 3 csrf token issue\nersincebi\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n34\nHow do I connect AWS API Gateway to an interna...\nVince_DC\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n35\nhow to move to new line in textbox after secon...\nshtela\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n36\nField fo start process error reports when auto...\nReda Kassem Elsaper\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n37\nHow to call toplevel window from the rootwindow\nHans Nicholas\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n38\nCAN Bus on CAR Android\nbvs60\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n39\nMachine Learning, Clustering IN Elbow method\nMohammed Mateen uddin\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n40\nHow do I edit the Gui name of an existing Auto...\nV0RT3X\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n41\nAndroid CookieManager returns null on API leve...\nPisoj\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n42\nPlaid connection returning Failed to establish...\nRick Walter\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n43\nHow do I connect from Docker Container to a Fl...\nYabukiman\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n8\\nviews\\n\n\n\n44\nHow to change the text insert after every sear...\nazbcexds\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n9\\nviews\\n\n\n\n45\nHow to read hexadecimal registers from a Modbu...\nEarnest Adeniyi\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n10\\nviews\\n\n\n\n46\nI need to create a streaming app Where videos ...\nabuzar zaidi\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n47\nhow to install caffe within google colab\nIsraa\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n48\nWhat python library is PIP using to display th...\nfrantizek\n\\n0\\nvotes\\n\n\\n1\\nanswer\\n\n\\n9\\nviews\\n\n\n\n49\nError in `n()`: ! Must only be used inside dat...\nYao Yao\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n9\\nviews\\n"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Learn with me",
    "section": "",
    "text": "This will brings you value:\n\n\n   \n     \n     Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\nPython\n\n\nWeb scraping\n\n\n\nPython offers us multiple battle tested solutions to tackle this problem properly.\n\n\n\nAlexandro Disla\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the pandas framework\n\n\n\nPython\n\n\nPandas\n\n\nEDA\n\n\n\nStandard framework for data wrangling and analysis\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the polars framework\n\n\n\nPython\n\n\nPolars\n\n\nEDA\n\n\n\nRobust framework for data wrangling compatible with pandas and any data visualization libraries\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeoanalysis\n\n\n\nPython\n\n\nR\n\n\nGeoAnalysis\n\n\n\nWe have multiple solutions available with R and Python. Let’s explore them.\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n  Don't forget to take a good cup of cofee\n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/photography/index.html",
    "href": "old_version/photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)\n\n\n\n\n Back to top"
  }
]
[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/projects/index.html",
    "href": "old_version/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-languages-of-middle-earth",
    "href": "old_version/projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "old_version/projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "blog/posts/geoanalysis.html",
    "href": "blog/posts/geoanalysis.html",
    "title": "geoanalysis",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda_rs.html",
    "href": "blog/posts/eda_rs.html",
    "title": "The powers of the polars framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "The ingredients\n\n\n\nData Analysis & Visualization\n\n\nBackend Services\n\n\nScraping & Automation\n\n\n\n \n  \n   \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n  \n    \n     Upwork\n  \n\n\n\n\nWelcome to my website!\nI am a disciplined, detail-oriented person with good communication skills and appropriate training and work experience. My goal is to be highly efficient and deliver products that meet customer needs and requests.\nI have Graduated from a school of applied economics and statistics. I have multiples of experiences in the fields of data analysis, modeling and backend engineering. In my daily job, I do exploratory data analysis to ensure the data quality of the multiple sources of data within the organization.I submit the bad data to be corrected by the data entry team or the IT team. I build automation reports that met the specified requirements. These reports are a combination of sql (or mongo queries), sqlalchemy, pymongo, pandas (or polars) and tidyverse (if needed). The goal is to present the results in the required format (word, excell, powerpoint, visualization) or the required context (a python/R dashboard, a web services). I dabbled with wordpress to serve the needs of my clients.\nBelow is a detailed list of my skills.\n\nQuarto and Rmarkdown (an open-source scientific and technical publishing system)\nMicrosoft Excel, Jupyter Notebook, Polars & Pandas, Seaborn & GGplot\nGeopandas, folium and leaflet for GeoAnalysis\nWeb scraping with scrapy and beautifulSoup\nWeb automation with Selenium\nShiny, streamlit, dash for all in one dashboard solutions\nWordPress (website design and development)\nSQL scripts\nbackend development (fastapi, express, flask,Restapi, graphql, web socket)\n\nI’d like to make it clear that for me, it’s a priviledge to be here to tackle and solve your problems. It is much more than a way to earn money. It’s also my passion. So I’ll do my best in every detail of every order, because the customer’s ultimate satisfaction is my priority.\nPlease don’t hesitate to contact me and I’ll be delighted to work with you.\nThank you in advance for choosing me.\nYours sincerely\nAlexandro Disla\nPlease feel free to contact me if you have any questions or would like to discuss potential projects.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/eda.html",
    "href": "blog/posts/eda.html",
    "title": "The powers of the pandas framework",
    "section": "",
    "text": "#TODO get the article up and running\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/posts/web_scraping.html",
    "href": "blog/posts/web_scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.\nIf you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own and make sure that you’re not violating any Terms of Service before you start a large-scale project.\n\n\nSay you’re a surfer, both online and in real life, and you’re looking for employment. However, you’re not looking for just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way!\nThere’s a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You think about checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time.\nThankfully, the world offers other ways to apply that surfer’s mindset! Instead of looking at the job site every day, you can use Python to help automate your job search’s repetitive parts. Automated web scraping can be a solution to speed up the data collection process. You write your code once, and it will get the information you want many times and from many pages.\nIn contrast, when you try to get the information you want manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and repetition.\nThere’s so much information on the Web, and new information is constantly added. You’ll probably be interested in at least some of that data, and much of it is just out there for the taking. Whether you’re actually on the job hunt or you want to download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals.\n\n\n\n7 Best Python Libraries For Web Scraping\n\n\nBeautifulSoup Features of BeautifulSoup Pros of BeautifulSoup Cons of BeautifulSoup Scrapy Features of Scrapy Pros of Scrapy Cons of Scrapy Selenium Features of Selenium Pros of Selenium Cons of Selenium Requests Features of Requests Pros of Requests Cons of Requests Urllib3 Features of urllib3 Pros of urllib3 Cons of urllib3 Lxml Features of LXML Pros of LXML Cons of LXML MechanicalSoup Features of MechanicalSoup Pros of MechanicalSoup Cons of MechanicalSoup\n\n\n\nSome website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\nWhen you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\nThe front-end presentation of a site might change often, but such a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data.\nHowever, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality.\nThe approach and tools you need to gather information using APIs are outside the scope of this tutorial. To learn more about it, check out API Integration in Python.\n\nNow let’s run a project with Beautiful Soup and scrapy"
  },
  {
    "objectID": "blog/posts/web_scraping.html#the-need-for-web-scraping",
    "href": "blog/posts/web_scraping.html#the-need-for-web-scraping",
    "title": "Web Scraping",
    "section": "",
    "text": "Say you’re a surfer, both online and in real life, and you’re looking for employment. However, you’re not looking for just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way!\nThere’s a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You think about checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time.\nThankfully, the world offers other ways to apply that surfer’s mindset! Instead of looking at the job site every day, you can use Python to help automate your job search’s repetitive parts. Automated web scraping can be a solution to speed up the data collection process. You write your code once, and it will get the information you want many times and from many pages.\nIn contrast, when you try to get the information you want manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and repetition.\nThere’s so much information on the Web, and new information is constantly added. You’ll probably be interested in at least some of that data, and much of it is just out there for the taking. Whether you’re actually on the job hunt or you want to download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals."
  },
  {
    "objectID": "blog/posts/web_scraping.html#available-options-and-alternatives",
    "href": "blog/posts/web_scraping.html#available-options-and-alternatives",
    "title": "Web Scraping",
    "section": "",
    "text": "7 Best Python Libraries For Web Scraping\n\n\nBeautifulSoup Features of BeautifulSoup Pros of BeautifulSoup Cons of BeautifulSoup Scrapy Features of Scrapy Pros of Scrapy Cons of Scrapy Selenium Features of Selenium Pros of Selenium Cons of Selenium Requests Features of Requests Pros of Requests Cons of Requests Urllib3 Features of urllib3 Pros of urllib3 Cons of urllib3 Lxml Features of LXML Pros of LXML Cons of LXML MechanicalSoup Features of MechanicalSoup Pros of MechanicalSoup Cons of MechanicalSoup\n\n\n\nSome website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\nWhen you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\nThe front-end presentation of a site might change often, but such a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data.\nHowever, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality.\nThe approach and tools you need to gather information using APIs are outside the scope of this tutorial. To learn more about it, check out API Integration in Python.\n\nNow let’s run a project with Beautiful Soup and scrapy"
  },
  {
    "objectID": "blog/posts/web_scraping.html#bs4",
    "href": "blog/posts/web_scraping.html#bs4",
    "title": "Web Scraping",
    "section": "BS4",
    "text": "BS4\n\n\nCode\nfrom requests import get\nfrom bs4 import BeautifulSoup, ResultSet\nfrom pandas import DataFrame\nfrom typing import List,Dict, Any\n\n\nThe scraping process of fake-jobs website:\n\n\nCode\nurl = \"https://stackoverflow.com/questions\"\nresponse = get(url)\n\n# Check if the request was successful\nif response.status_code != 200:\n    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n    exit()\n\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n\n# Just in case you need the html file\n\"\"\" with open(\"stackoverflow_data.html\",'wb') as file:\n    file.write(\n        soup.find('div',id=\"mainbar\").find('div',class_=\"flush-left\").prettify(\"utf-8\")\n    )\n\"\"\"\n\n\nquestions = soup.select('.s-post-summary.js-post-summary')\n\n\n\n\nCode\ndef questions_handler(questions: ResultSet):\n    data={\n        'title': [],\n        'user': [],\n        'vote_count': [],\n        'answer_count': [],\n        'view_count': []\n    }\n    for question in questions:\n        title = question.select_one('.s-link').getText()\n        user = question.select_one('.s-user-card--link a').getText()\n        vote_count = question.select_one('.s-post-summary--stats-item__emphasized').getText()\n        answer_count = question.select_one('.s-post-summary--stats-item:nth-child(2)').getText()\n        view_count = question.select_one('.s-post-summary--stats-item:nth-child(3)').getText()\n        if title:\n            data[\"title\"].append(title)\n        if user:\n            data[\"user\"].append(user)\n        if vote_count:\n            data[\"vote_count\"].append(vote_count)\n        if answer_count:\n            data[\"answer_count\"].append(answer_count)\n        if view_count:\n            data[\"view_count\"].append(view_count)\n    return data\n\n\n\n\nCode\ndef build_dataframe(questions_set: ResultSet):\n    data = questions_handler(questions=questions_set)\n    if not data[\"title\"]:\n        return DataFrame({\n            \"message\": [\"The Stackoverflow page has been changed\"],\n            \"action\": [\"correct the scraper\"]\n        })\n    return DataFrame({\n        \"titles\": data[\"title\"],\n        \"users\": data[\"user\"],\n        \"vote_counts\": data[\"vote_count\"],\n        \"answer_counts\": data[\"answer_count\"],\n        \"view_counts\": data[\"view_count\"],\n    })\n\n\nMake a DataFrame:\n\n\nCode\nbuild_dataframe(questions)\n\n\n\n\n\n\n\n\n\ntitles\nusers\nvote_counts\nanswer_counts\nview_counts\n\n\n\n\n0\nStackExchange HASH conversion to and from C# o...\nuser2981411\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n2\\nviews\\n\n\n\n1\nDeleting blobs from a sub-folder within a cont...\nDepressio\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n2\nError on local host Chrome with React project,...\nJaal Audin\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n3\nFlutter Video Player streaming videos error on...\nNooreldin Elsayed\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n4\nAppSync VTL: Returning From Request Resolver W...\nJTAN\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n5\nKafka C#: Error occured: Subscribed topic not ...\nanon\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n6\nHow to diff a very long csv with a very long txt\nFélix\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n10\\nviews\\n\n\n\n7\nIs ToolReview.org the ultimate guide to assist...\nMia Lewis\n\\n-2\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n8\nWhy does my pygbag pygame application have lag...\nLleyton Bell\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n9\nWhy is my first asm instruction located at 0x8...\nSynthwave09\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n10\nWhy does kerberos suddenly stop working with V...\nCharlie Parker\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n11\nSame Framework for multiple target and differe...\nAhmed ibrahim\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n12\nUnable to control full screen menu bar visibil...\nkrzychek\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n13\nHow should MqttClient lifecycle be managed?\nJoe Schrag\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n14\nAuth0 External Provider\nxxxyyy\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n15\nPassing a function to a react-router action in...\nNeil E\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n16\nI'm getting an error flag in the script tag of...\nisaiahAndrew\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n17\nHow do I change the sharing settings for a ser...\nRyan Tanner-Read\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n18\nHow to disable new Logcat in Android Studio Gi...\nDoron Yakovlev Golani\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n3\\nviews\\n\n\n\n19\nStreamlining Multiple Open Scripts in RStudio ...\nTarJae\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n20\nPython Mypy fails on Decorator [duplicate]\nMicah Pearce\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n21\nIntegration Test of a Controller with MockMVC ...\nGabriel\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n22\nHow do I find the key with the highest value i...\nRed1269 _\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n9\\nviews\\n\n\n\n23\nSelect default tab in nav-tab\nphp\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n8\\nviews\\n\n\n\n24\nHow to show images and/or files at same time w...\nDaniel\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n25\nnavigator.pop() instead of closing a alertDial...\nDizzyDagem\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n26\nAllowing only one object and disabling all ass...\nniil87\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n13\\nviews\\n\n\n\n27\nCan you inherit the default properties of anot...\nuser22009348\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n12\\nviews\\n\n\n\n28\nType safety for REST endpoints\nCilenco\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n29\n.Net Core Web API project can't connect to SQL...\nAbdi\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n30\nHow to insert a Python script into a website w...\nMaksiks\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n31\nBeautifulsoup append ignores namespace (xml)\notto\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n32\nAccessing a Kusto extract_all(...) result whic...\nredgiant\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n33\nDynamically populate masonry view on HTML web ...\nLdweller\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n34\nAdding new layer to existing docker image\nJakeUT\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n35\nCode structure and responsibility in hexagonal...\nKarol\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n9\\nviews\\n\n\n\n36\nLanguage for Turing Machine\nSilver Ms\n\\n0\\nvotes\\n\n\\n1\\nanswer\\n\n\\n12\\nviews\\n\n\n\n37\nMask the account numbers in a file in Unix\nDito\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n38\ntrailing numbers after converting negative to ...\nneta cohen\n\\n1\\nvote\\n\n\\n1\\nanswer\\n\n\\n19\\nviews\\n\n\n\n39\nNSWorkspace.shared.open(_:) not working as exp...\nuser22260523\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n\n\n\n40\nHow to find specific values in a nc file?\nmadison powell\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n41\nSegmentation error in php extension when using...\nMohsen Davari\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n42\nRewriterule to remove .php and .html in .htacc...\nlololomick\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n43\nFlutter || Check condition inside ListView.bui...\nPhoenix\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n44\nA problem occurred running the Server launcher...\nCrustyBuns\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n6\\nviews\\n\n\n\n45\nHow to write junit tests for void method which...\nBodapati Srinu\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n46\nWhat is the B2C Custom Policy Syntax to send q...\nIkeem Wilson\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n47\nHow to leave a PR review as a GitHub app?\nEric\n\\n-1\\nvotes\\n\n\\n0\\nanswers\\n\n\\n5\\nviews\\n\n\n\n48\nSVG element onClick event with z index\nOsman Mamun\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n4\\nviews\\n\n\n\n49\nHow to mock the v3 aws-sdk's paginators?\nghidalgo\n\\n0\\nvotes\\n\n\\n0\\nanswers\\n\n\\n7\\nviews\\n"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Learn with me",
    "section": "",
    "text": "This will brings you value:\n\n\n   \n     \n     Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\nPython\n\n\nWeb scraping\n\n\n\nPython offers us multiple battle tested solutions to tackle this problem properly.\n\n\n\nAlexandro Disla\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the pandas framework\n\n\n\nPython\n\n\nPandas\n\n\nEDA\n\n\n\nStandard framework for data wrangling and analysis\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe powers of the polars framework\n\n\n\nPython\n\n\nPolars\n\n\nEDA\n\n\n\nRobust framework for data wrangling compatible with pandas and any data visualization libraries\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeoanalysis\n\n\n\nPython\n\n\nR\n\n\nGeoAnalysis\n\n\n\nWe have multiple solutions available with R and Python. Let’s explore them.\n\n\n\nAlexandro Disla\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n  Don't forget to take a good cup of cofee\n\n\n\n\n Back to top"
  },
  {
    "objectID": "old_version/photography/index.html",
    "href": "old_version/photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)\n\n\n\n\n Back to top"
  }
]